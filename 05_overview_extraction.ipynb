{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47d7336",
   "metadata": {},
   "source": [
    "# Extraindo Gênero de Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2389ea5",
   "metadata": {},
   "source": [
    "## 1 - Bibliotecas e Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01fcf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "     -------------------------------------- 494.8/494.8 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "     ---------------------------------------- 134.8/134.8 kB ? eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "     -------------------------------------- 561.5/561.5 kB 8.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-win_amd64.whl (26.2 MB)\n",
      "     --------------------------------------- 26.2/26.2 MB 38.4 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.7/64.7 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "     ------------------------------------- 193.6/193.6 kB 11.5 MB/s eta 0:00:00\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.15-cp310-cp310-win_amd64.whl (452 kB)\n",
      "     -------------------------------------- 452.3/452.3 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "     ---------------------------------------- 43.8/43.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.4-cp310-cp310-win_amd64.whl (46 kB)\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "     ---------------------------------------- 86.8/86.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, requests, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 6.0.0\n",
      "    Uninstalling pyarrow-6.0.0:\n",
      "      Successfully uninstalled pyarrow-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acesso negado: 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\~yarrow\\\\arrow.dll'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eaabb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "df = pd.read_csv('data/df_split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd61e1b",
   "metadata": {},
   "source": [
    "## 2 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f707e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Main_Genre'] = df['Genre'].str.split(',').str[0]\n",
    "\n",
    "# Filtrar gêneros raros (menos de 5 filmes)\n",
    "genre_counts = df['Main_Genre'].value_counts()\n",
    "genres_to_keep = genre_counts[genre_counts >= 5].index\n",
    "df_filtered = df[df['Main_Genre'].isin(genres_to_keep)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6a7a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_filtered['genre_label'] = le.fit_transform(df_filtered['Main_Genre'])\n",
    "\n",
    "# Divisão treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_filtered['Overview'],\n",
    "    df_filtered['genre_label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_filtered['genre_label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4740c9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d0b9ab41e14b81bf5efa53caea74c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a50f2d768e4d52a351c3c04dfd6857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f0383f63b34dd7a1bc336429856a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ccc4490629450e9f93725945993ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizador DistilBERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92e59881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = MovieDataset(train_encodings, list(y_train))\n",
    "test_dataset = MovieDataset(test_encodings, list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4fcb9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03e3c93fce041588fcd6c8ebf9b4e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(df_filtered['genre_label'].unique())\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5de02866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,           # menos epochs\n",
    "    per_device_train_batch_size=8, # batch menor\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88d5870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 198\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36ed1f38bcc484d9e97a511bae423ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.29292929292929293\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.00      0.00      0.00        35\n",
      "   Adventure       0.00      0.00      0.00        14\n",
      "   Animation       0.00      0.00      0.00        17\n",
      "   Biography       0.00      0.00      0.00        18\n",
      "      Comedy       0.00      0.00      0.00        31\n",
      "       Crime       0.00      0.00      0.00        21\n",
      "       Drama       0.29      1.00      0.45        58\n",
      "      Horror       0.00      0.00      0.00         2\n",
      "     Mystery       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.29       198\n",
      "   macro avg       0.03      0.11      0.05       198\n",
      "weighted avg       0.09      0.29      0.13       198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
